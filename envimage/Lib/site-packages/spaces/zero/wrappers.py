"""
"""
from __future__ import annotations

import multiprocessing
import os
import signal
import traceback
from concurrent.futures import ThreadPoolExecutor
from contextvars import copy_context
from datetime import timedelta
from functools import partial
from functools import wraps
from multiprocessing import Process
from pickle import PicklingError
from queue import Empty
from queue import Queue as ThreadQueue
from threading import Thread
from typing import TYPE_CHECKING
from typing import Callable
from typing import Generator
from typing import Generic
from typing_extensions import assert_never

import gradio as gr
import psutil

from ..utils import debug
from ..utils import drop_params
from ..utils import gradio_request_var
from ..utils import SimpleQueue as Queue
from . import client
from . import torch
from .api import AllowToken
from .api import NvidiaIndex
from .api import NvidiaUUID
from .gradio import GradioPartialContext
from .gradio import patch_gradio_queue
from .gradio import try_process_queue_event
from .tqdm import remove_tqdm_multiprocessing_lock
from .types import * # TODO: Please don't do that

GENERATOR_GLOBAL_TIMEOUT = 20 * 60

Process = multiprocessing.get_context('spawn').Process
forked = False

class Worker(Generic[Res]):
    process: Process
    arg_queue: Queue[tuple[Params, GradioPartialContext]]
    res_queue: Queue[Res | None]
    _sentinel: Thread

    def __init__(
        self,
        target: Callable[[
            Queue[tuple[Params, GradioPartialContext]],
            Queue[Res | None],
            AllowToken | None,
            NvidiaUUID,
            list[int],
        ], None],
        allow_token: str | None,
        nvidia_uuid: str,
    ):
        self._sentinel = Thread(target=self._close_on_exit)
        self.arg_queue = Queue()
        self.res_queue = Queue()
        fds = [c.fd for c in psutil.Process().connections()]
        args = self.arg_queue, self.res_queue, allow_token, nvidia_uuid, fds
        if TYPE_CHECKING:
            target(*args)
        self.process = Process(
            target=target,
            args=args,
            daemon=True,
        )
        self.process.start()
        self._sentinel.start()

    def _close_on_exit(self):
        self.process.join()
        self.res_queue.put(None)

def worker_init(
    res_queue: Queue[RegularResQueueResult | None] | Queue[GeneratorResQueueResult | None],
    allow_token: str | None,
    nvidia_uuid: str,
    fds: list[int],
) -> None | ExceptionResult:
    try: # Unrecoverable init part
        if allow_token is not None:
            client.allow(allow_token)
        torch.unpatch()
        torch.move(nvidia_uuid)
        patch_gradio_queue(res_queue)
    except Exception as e: # pragma: no cover
        traceback.print_exc()
        return ExceptionResult(e)
    try:
        remove_tqdm_multiprocessing_lock()
    except Exception: # pragma: no cover
        print("Error while trying to remove tqdm mp_lock:")
        traceback.print_exc()
    for fd in fds:
        try:
            os.close(fd)
        except Exception as e: # pragma: no cover
            if isinstance(e, OSError) and e.errno == 9:
                continue
            traceback.print_exc()
            return ExceptionResult(e)

def regular_function_wrapper(
    task: Callable[Param, Res],
    duration: timedelta | None,
) -> Callable[Param, Res]:

    request_var = gradio_request_var()
    workers: dict[NvidiaIndex, Worker[RegularResQueueResult[Res]]] = {}
    task_id = id(task)

    @wraps(task)
    def gradio_handler(*args: Param.args, **kwargs: Param.kwargs) -> Res:

        if forked:
            return task(*args, **kwargs)

        request = request_var.get()
        schedule_response = client.schedule(task_id=task_id, request=request, duration=duration)
        allow_token = schedule_response.allowToken
        nvidia_index = schedule_response.nvidiaIndex
        nvidia_uuid = schedule_response.nvidiaUUID
        release = partial(client.release, task_id=task_id, nvidia_index=nvidia_index)

        worker = workers.get(nvidia_index)
        if worker is None or not worker.process.is_alive():
            worker = Worker(thread_wrapper, allow_token, nvidia_uuid)
            workers[nvidia_index] = worker

        try:
            worker.arg_queue.put(((args, kwargs), GradioPartialContext.get()))
        except PicklingError:
            release(fail=True)
            # TODO: Better error message (check what arg / kwarg is problematic ?)
            raise

        while True:
            res = worker.res_queue.get()
            if res is None:
                release(fail=True, allow_404=True)
                raise gr.Error("GPU task aborted")
            if isinstance(res, ExceptionResult):
                release(fail=True)
                raise res.value
            if isinstance(res, OkResult):
                release()
                return res.value
            if isinstance(res, GradioQueueEvent):
                try_process_queue_event(res.method_name, *res.args, **res.kwargs)
                continue
            assert_never(res)

    def thread_wrapper(
        arg_queue: Queue[tuple[Params, GradioPartialContext]],
        res_queue: Queue[RegularResQueueResult[Res] | None],
        allow_token: str | None,
        nvidia_uuid: str,
        fds: list[int],
    ):
        global forked
        forked = True
        if (res := worker_init(
            res_queue=res_queue,
            allow_token=allow_token,
            nvidia_uuid=nvidia_uuid,
            fds=fds,
        )) is not None: # pragma: no cover
            res_queue.put(res)
            return
        signal.signal(signal.SIGTERM, drop_params(arg_queue.close))
        while True:
            try:
                (args, kwargs), gradio_context = arg_queue.get()
            except OSError:
                break
            GradioPartialContext.apply(gradio_context)
            context = copy_context()
            with ThreadPoolExecutor() as executor:
                future = executor.submit(context.run, task, *args, **kwargs) # type: ignore
            try:
                res = future.result()
            except Exception as e:
                traceback.print_exc()
                res = ExceptionResult(e)
            else:
                res = OkResult(res)
            try:
                res_queue.put(res)
            except PicklingError as e:
                res_queue.put(ExceptionResult(e))

    # https://github.com/python/cpython/issues/91002
    if not hasattr(task, '__annotations__'):
        gradio_handler.__annotations__ = {}

    return gradio_handler

# Please repeat the adaptation process for the generator_function_wrapper function and any other parts if needed.
def generator_function_wrapper(task, duration):
    def wrapper(*args, **kwargs):
        # Your code to handle the generator function goes here.
        # Example of wrapping a generator:
        gen = task(*args, **kwargs)
        try:
            while True:
                data = next(gen)
                # Process data or handle GPU tasks
                yield data
        except StopIteration:
            return
        except Exception as e:
            raise RuntimeError("An error occurred in the GPU-enabled generator") from e
    return wrapper